apply plugin: 'java'
apply plugin: 'download-task'

def configDir = new File(rootDir, 'config')
def cdh4rel = 'cdh4.3.0'
def hadoop_version = project.getProperties().get('hadoop_version', '2.2')
def versionMap = [
        '0.23': '0.23.10',
        '1.0' : '1.0.4',
        '1.1' : '1.1.2',
        'cdh4': "2.0.0-${cdh4rel}",
        '2.2' : '2.2.0'
].withDefault {
    throw new GradleException("Unknown hadoop version: ${hadoop_version}")
}
buildscript {
    repositories {
        mavenLocal()
        mavenCentral()
        maven { url 'https://oss.sonatype.org/content/repositories/snapshots' }
    }
    dependencies {
        classpath 'me.trnl:clirr-gradle-plugin:0.4'
        classpath 'com.antwerkz.github:github-release-gradle-plugin:1.0.0-RC3'
        classpath 'org.gradle.api.plugins:gradle-nexus-plugin:0.3'
        classpath 'de.undercouch:gradle-download-task:0.5'
    }
}

task wrapper(type: Wrapper) {
    gradleVersion = '1.10'
}

configure(subprojects) {
    apply plugin: 'java'
    apply plugin: 'idea'
    apply plugin: 'eclipse'
    apply plugin: 'checkstyle'
    apply plugin: 'findbugs'

    repositories {
        mavenCentral()
        mavenLocal()
        maven { url "https://repository.cloudera.com/artifactory/cloudera-repos/" }
    }

    group = 'org.mongodb'
    version = '1.2.1-SNAPSHOT'

    sourceCompatibility = JavaVersion.VERSION_1_6
    targetCompatibility = JavaVersion.VERSION_1_6

    dependencies {
        compile "org.mongodb:mongo-java-driver:2.11.3"

        testCompile 'junit:junit:4.11'
        testCompile 'org.hamcrest:hamcrest-all:1.3'
    }

    /* Compiling */
    tasks.withType(AbstractCompile) {
        options.encoding = 'ISO-8859-1'
        options.fork = true
        options.debug = true
//        options.compilerArgs = ['-Xlint:all', '-Xlint:-options']
    }

    project.ext.buildingWith = { n ->
        project.hasProperty(n) && project.property(n).toBoolean()
    }

    /* Testing */
    tasks.withType(Test) {
        maxParallelForks = 1

        beforeTest { descr ->
            logger.info("[Test ${descr.className} > ${descr.name}]")
        }
    }

    task testAll(dependsOn: tasks.withType(Test))
    check.dependsOn testAll

    /* Code quality */
    checkstyle {
        configFile = new File("$configDir/checkstyle.xml")
    }

    checkstyleTest {
        classpath += configurations.compile
    }

    checkstyleMain {
        classpath += configurations.compile
    }

    findbugs {
        excludeFilter = new File("$configDir/findbugs-exclude.xml")
        sourceSets = [sourceSets.main]
    }

    tasks.withType(FindBugs) {
        reports {
            xml.enabled = project.buildingWith('xmlReports.enabled')
            html.enabled = !project.buildingWith('xmlReports.enabled')
        }
    }

    jar {
        classifier = "hadoop_${hadoop_version}"
    }

    task listDependencies << {
        configurations.compile.each { File file -> println file }
    }
}

configure(subprojects.findAll { it.name.contains('examples/') }) {
    def exampleName = project.name.split('/')[1]
    jar {
        baseName = exampleName
    }
    group += ".mongo-hadoop-examples"

}

project(":core") {
    archivesBaseName = "mongo-hadoop-core"

    dependencies {
        switch (hadoop_version) {
            case ("0.23"):
                compile "org.apache.hadoop:hadoop-common:${versionMap[hadoop_version]}"
                mapReduceDeps(it, versionMap[hadoop_version])
                break
            case ("1.0"):
                compile "org.apache.hadoop:hadoop-core:${versionMap[hadoop_version]}"
                break
            case ("1.1"):
                compile "org.apache.hadoop:hadoop-core:${versionMap[hadoop_version]}"
                break
            case ("cdh4"):
                compile "org.apache.hadoop:hadoop-common:${versionMap[hadoop_version]}"
                mapReduceDeps(it, versionMap[hadoop_version])
                break
            case ("2.2"):
                compile "org.apache.hadoop:hadoop-common:${versionMap[hadoop_version]}"
                mapReduceDeps(it, versionMap[hadoop_version])
                break
        }
    }
}

project(":hive") {
    archivesBaseName = "mongo-hadoop-hive"
    dependencies {
        compile project(':core')

        compile "org.apache.hive:hive-exec:0.10.0"
        if (hadoop_version == "cdh4") {
            compile "org.apache.hive:hive-serde:0.10.0-cdh4.2.0"
        } else {
            compile "org.apache.hive:hive-serde:0.10.0"
        }
    }
}

project(":pig") {
    archivesBaseName = "mongo-hadoop-pig"
    dependencies {
        compile project(':core')
        compile "org.antlr:antlr:3.4"

        if (hadoop_version == "cdh4") {
            compile "org.apache.pig:pig:0.10.0-cdh4.2.0"
        } else {
            compile "org.apache.pig:pig:0.9.2"
        }
    }

    jar {
        from project(':core').sourceSets.main.output
        from sourceSets.main.output

        configurations.compile.filter {
            it.name.startsWith('mongo-java-driver')
        }.each {
            from zipTree(it)
        }
    }

}

project(":streaming") {
    archivesBaseName = "mongo-hadoop-streaming"

    dependencies {
        compile project(':core')
        compile "org.apache.hadoop:hadoop-streaming:${versionMap[hadoop_version]}"
    }

    jar {
        onlyIf { hadoop_version != "1.0" }

        from project(':core').sourceSets.main.output
        from sourceSets.main.output

        configurations.compile.filter {
            it.name.startsWith('mongo-java-driver')
        }.each {
            from zipTree(it)
        }
    }
}

project(":flume") {
    dependencies {
        compile project(':core')
        compile "com.cloudera:flume-core:0.9.4-cdh3u3"
    }
}

/*
project(":examples/ufo_sightings") {
    dependencies {
        compile project(':core')
    }
}
*/

project(":examples/treasury_yield") {
    uploadArchives.onlyIf { false }
    dependencies {
        compile project(':core')
    }
}

project(":examples/enron") {
    uploadArchives.onlyIf { false }
    dependencies {
        compile project(':core')
    }
}

project(":examples/sensors") {
    uploadArchives.onlyIf { false }
    dependencies {
        compile project(':core')
    }
}

task historicalYield() {
    doLast {
        exec() {
            commandLine "mongoimport", "-d", "mongo_hadoop", "-c", "yield_historical.in", "--drop",
                        "examples/treasury_yield/src/main/resources/yield_historical_in.json"
        }

        hadoop("examples/treasury_yield/build/libs/treasury_yield-${project(':core').version}-hadoop_${hadoop_version}.jar",
               "com.mongodb.hadoop.examples.treasury.TreasuryYieldXMLConfig",

               ["mongo.job.input.format=com.mongodb.hadoop.MongoInputFormat",
                "mongo.input.uri=mongodb://localhost:27017/mongo_hadoop.yield_historical.in",
                // "mongo.job.input.format=com.mongodb.hadoop.BSONFileInputFormat",
                // "mapred.input.dir=file:///Users/mike/dump/mongo_hadoop/yield_historical.in.bson",

                "mongo.job.mapper=com.mongodb.hadoop.examples.treasury.TreasuryYieldMapper",
                "mongo.job.reducer=com.mongodb.hadoop.examples.treasury.TreasuryYieldReducer",

                "mongo.job.mapper.output.key=org.apache.hadoop.io.IntWritable",
                "mongo.job.mapper.output.value=org.apache.hadoop.io.DoubleWritable",
                "mongo.job.output.key=org.apache.hadoop.io.IntWritable",
                "mongo.job.output.value=com.mongodb.hadoop.io.BSONWritable",

                "mongo.output.uri=mongodb://localhost:27017/mongo_hadoop.yield_historical.out",
                "mongo.job.output.format=com.mongodb.hadoop.MongoOutputFormat",
                // "mapred.output.dir=file:///tmp/yield_historical_out.bson",
                // "mongo.job.output.format=com.mongodb.hadoop.BSONFileOutputFormat"
               ])
    }
}

task sensorData() {
    doLast {
        hadoop("examples/sensors/build/libs/sensors-${project(':core').version}-hadoop_${hadoop_version}.jar",
               "com.mongodb.hadoop.examples.sensors.Sensors",

               ["mongo.job.input.format=com.mongodb.hadoop.MongoInputFormat",
                "mongo.input.uri=mongodb://localhost:27017/mongo_hadoop.devices",

                "mongo.job.mapper=com.mongodb.hadoop.examples.sensors.DeviceMapper",
                "mongo.job.reducer=com.mongodb.hadoop.examples.sensors.DeviceReducer",

                "mongo.job.mapper.output.key=org.apache.hadoop.io.Text",
                "mongo.job.mapper.output.value=org.apache.hadoop.io.Text",
                "mongo.job.output.key=org.apache.hadoop.io.IntWritable",
                "mongo.job.output.value=com.mongodb.hadoop.io.BSONWritable",

                "mongo.output.uri=mongodb://localhost:27017/mongo_hadoop.logs_aggregate",
                "mongo.job.output.format=com.mongodb.hadoop.MongoOutputFormat"
               ])

        hadoop("examples/sensors/build/libs/sensors-${project(':core').version}-hadoop_${hadoop_version}.jar",
               "com.mongodb.hadoop.examples.sensors.Sensors",

               ["mongo.input.uri=mongodb://localhost:27017/mongo_hadoop.logs",
                "mongo.job.input.format=com.mongodb.hadoop.MongoInputFormat",
                "mongo.job.mapper=com.mongodb.hadoop.examples.sensors.LogMapper",
                "mongo.job.reducer=com.mongodb.hadoop.examples.sensors.LogReducer",
                "mapreduce.combiner.class=com.mongodb.hadoop.examples.sensors.LogCombiner",
                "mongo.job.combiner=com.mongodb.hadoop.examples.sensors.LogCombiner",
                "io.sort.mb=100",

                "mongo.job.output.key=org.apache.hadoop.io.Text",
                "mongo.job.output.value=org.apache.hadoop.io.IntWritable",

                "mongo.output.uri=mongodb://localhost:27017/mongo_hadoop.logs_aggregate",
                "mongo.job.output.format=com.mongodb.hadoop.MongoOutputFormat"
               ])
    }
}

task enronEmails() {
    doLast {
        download {
            src 'https://s3.amazonaws.com/mongodb-enron-email/enron_mongo.tar.bz2'
            dest 'examples/data/'
            onlyIfNewer true
        }
        if (!new File('examples/data/dump').exists()) {
            println "extracting email data"
            copy {
                from(tarTree('examples/data/enron_mongo.tar.bz2'))
                into 'examples/data'
            }
        }
        
        exec() {
            commandLine "mongorestore", "-v", "-d", "mongo_hadoop", "--drop", "examples/data/dump/enron_mail"
        }

        hadoop("examples/enron/build/libs/enron-${project(':core').version}-hadoop_${hadoop_version}.jar",
               "com.mongodb.hadoop.examples.enron.EnronMail",

               ["mongo.job.input.format=com.mongodb.hadoop.MongoInputFormat",
                "mongo.input.uri=mongodb://localhost:27017/mongo_hadoop.messages",

                "mongo.input.split_size=64",

                "mongo.job.mapper=com.mongodb.hadoop.examples.enron.EnronMailMapper",
                "mongo.job.reducer=com.mongodb.hadoop.examples.enron.EnronMailReducer",
                //"mongo.job.combiner=com.mongodb.hadoop.examples.enron.EnronMailReducer",

                "mongo.job.output.key=com.mongodb.hadoop.examples.enron.MailPair",
                "mongo.job.output.value=org.apache.hadoop.io.IntWritable",

                "mongo.job.mapper.output.key=com.mongodb.hadoop.examples.enron.MailPair",
                "mongo.job.mapper.output.value=org.apache.hadoop.io.IntWritable",

                "mongo.output.uri=mongodb://localhost:27017/mongo_hadoop.message_pairs",
                "mongo.job.output.format=com.mongodb.hadoop.MongoOutputFormat"

               ])
    }
}
/*task ufoSightings() {
    doLast {
        exec() {
            commandLine "mongoimport", "-d", "mongo_hadoop", "-c", "ufo_sightings.in", "--drop",
                        "examples/ufo_sightings/src/main/resources/ufo_awesome.json"
        }
        
//        $HADOOP_HOME/bin/hadoop jar ./target/ufo-example_cdh4.3.0-1.1.0.jar -Dmongo.input.uri=mongodb://localhost:27017/test.ufo 
// -Dmongo.output.uri=mongodb://localhost:27017/test.ufo_out 

        hadoop("examples/ufo_sightings/build/libs/ufo_sightings-${project(':core').version}-hadoop_${hadoop_version}.jar",
               "com.mongodb.hadoop.examples.ufos.UfoSightings",

               ["mongo.job.input.format=com.mongodb.hadoop.MongoInputFormat",
                "mongo.input.uri=mongodb://localhost:27017/mongo_hadoop.ufo",
                // "mongo.job.input.format=com.mongodb.hadoop.BSONFileInputFormat",
                // "mapred.input.dir=file:///Users/mike/dump/mongo_hadoop/yield_historical.in.bson",

//                "mongo.output.uri=mongodb://localhost:27017/mongo_hadoop.ufo.out",
//                "mongo.job.output.format=com.mongodb.hadoop.MongoOutputFormat",

                "mongo.job.mapper=com.mongodb.hadoop.examples.ufos.UfoSightingsMapper",
                "mongo.job.reducer=com.mongodb.hadoop.examples.ufos.UfoSightingsReducer",

                "mongo.job.mapper.output.key=org.apache.hadoop.io.Text",
                "mongo.job.mapper.output.value=org.apache.hadoop.io.IntWritable",
                "mongo.job.output.key=org.apache.hadoop.io.Text",
                "mongo.job.output.value=com.mongodb.hadoop.io.IntWritable",

               ])
        
    }
}*/

def hadoop(jar, className, args) {
    def line = ["${System.env.HADOOP_HOME}/bin/hadoop",
                "jar", jar, className,
                //Split settings
                "-Dmongo.input.split_size=8",
                "-Dmongo.job.verbose=true",
    ]
    args.each {
        line << "-D${it}"
    }
    exec() {
        commandLine line
    }
}

def mapReduceDeps(it, version) {
    ["core", "common", "shuffle", "app", "jobclient"].each { module ->
        it.compile("org.apache.hadoop:hadoop-mapreduce-client-${module}:${version}") {
            exclude group: "org.apache.hadoop", module: "hadoop-hdfs"
        }
    }
}

apply from: 'gradle/maven-deployment.gradle'
